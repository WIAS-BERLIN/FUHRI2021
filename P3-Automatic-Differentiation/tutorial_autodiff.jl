### A Pluto.jl notebook ###
# v0.14.3

using Markdown
using InteractiveUtils

# ╔═╡ 5cb08a46-a0f0-11eb-3aaa-03f9763dcb75
begin
	#comment the next two lines in two install the packages from Pluto
	#using Pkg
	#Pkg.add(["PlutoUI","ForwardDiff","DiffResults","ReverseDiff"])
	using ForwardDiff
	using DiffResults
	using ReverseDiff
	using PlutoUI
	PlutoUI.TableOfContents()
end

# ╔═╡ d4c4e2f4-ba74-400d-aee1-d5ac2c63ee19
md"""

# Automatic Differentiation with Julia

## Introduction

This tutorial gives some first insight into the automatic differentiation features (mainly ForwardDiff.jl) of Julia. To run it properly you need to have installed the following packages:

- [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl)
- [DiffResults](https://github.com/JuliaDiff/DiffResults.jl)
- [ReverseDiff](https://github.com/JuliaDiff/ReverseDiff.jl)
- Pluto.jl and PlutoUI.jl

After all packages ar loaded you should see the table of contents on the right-hand side.

"""

# ╔═╡ b19235be-0338-4212-9d0a-5e45a1c866ea
md"""

## Forward differentiation via Dual Numbers

Forward differentiation is based on dual numbers. Dual numbers look like complex numbers as they are written as

$ z = a + b \epsilon \quad \text{where} \quad \epsilon^2 = 0 $

and have a 'real part' $a$ and a 'dual part' $b$ being the coefficient for the 'dual unit' $\epsilon$.

You can add and multiply them commutatively by

$ (a + b \epsilon) + (x + y \epsilon) = (a + x) + \epsilon (b + y) $

$ (a + b \epsilon) \cdot (x + y \epsilon) = (ax + 0) + \epsilon (ay + bx) $

"""

# ╔═╡ 688958d3-c39d-488c-9950-2c5fc83dee40
md"""

### Taylor series

Consider a (smooth) function $f : \mathbb{R} \rightarrow \mathbb{R}$ and its Taylor series expansion at some value $a$:

$ f(a+h) = f(a) + \sum_{n=1}^\infty \frac{f^{(n)}(a)}{n!} h^n $

Now, for $h = \epsilon$, due to $\epsilon^2 = 0$ one immediately obtains

$ f(a+h) = f(a) + f^{(1)}(a) \epsilon $

In other words: Evaluating the function in the dual number $(a + 1\epsilon)$ gives the dual number $(f(a) + f^{(1)}(a) \epsilon)$ as a result.

**Key point**: If the function $f$ can be also evaluated in dual numbers, one can obtain its derivatives 'automatically' on basis of the dual number operations above!
Due to Julia's on demand compilation and type dispatching, there will be two compiled versions of f(x<:Real), one for e.g. x::Float64 and one for the corressponding dual numbers (which is still a subtype of Real).

Details on the actual implementation of the dual numbers and its operation by operator overloading can be found in the [Documentation of ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable/dev/how_it_works/). Observe that DualNumbers are indeed a subtype of Real.

**Exercise 1**: Compute the derivative of $f(x) = (x-1)(x-2)$ at $x = 3$ by hand using dual numbers, i.e. compute $f(3 + \epsilon)$.

"""

# ╔═╡ c9656131-be14-48f3-9386-8320906ae3e5
begin
	Text("Reveal this cell completely to see the solution to the exercise above.")
	#
	# f(3+ϵ) = (2+ϵ)(1+ϵ) = 2 + 3ϵ + ϵ^2 = 3 + 3ϵ
	#
	# Hence, f(3) = 2 and df(3) = 3
	#
	# For arbitrary x one obtains
	# f(x+ϵ) = (x+ϵ-1)(x+ϵ-2) = x^2-3x+2 + ϵ(2x-3)
	#
	# Hence, df(x) = 2x-3
	#
end

# ╔═╡ 2029b8bd-408c-4e2f-a628-ee2a56eed43a
md"""

### Example 1 : 1D differentiation
Let's define some scalar-valued function $f : \mathbb{R} \rightarrow \mathbb{R}$ to test this:

"""

# ╔═╡ 605fa750-99c8-4ea4-928b-2e13110de9a8
function f(z::Real)
	return cos(z)*sin(z) + z^2 - 1 # change the function here if you like
end

# ╔═╡ 9153b452-f073-402d-ac68-a257cb8e7740
md"""

The first and second derivatives of our function f can be generated by the following lines:

"""

# ╔═╡ 624a73e1-3ee0-4e7e-a7ec-8d9146d2ac29
begin
	df(z) = ForwardDiff.derivative(f,z)
	d2f(z) = ForwardDiff.derivative(df,z)
end

# ╔═╡ 53a4910c-5e18-4d66-bc34-5712bd0934a9
md"""

Let's evaluate f and its derivatives at some point z.

"""

# ╔═╡ d0bdbfaf-9a8a-408c-aace-5ed53052ab4f
z = pi # change the evaluation point here if you like

# ╔═╡ e1bbb076-1ec4-4005-a69c-667e7fd0a35b
Text("  f(z) = $(f(z))\n df(z) = $(df(z))\nd2f(z) = $(d2f(z))")

# ╔═╡ 271511b3-c5e5-4ca2-ab79-5372b9f9a616
md"""

**Exercise 2.1** : Check what you have computed by hand in the dual number exercise above.

**Exercise 2.2** : Try discontinuous functions to see what happens, e.g. $f(z) = \lvert z \rvert$ around $z = 0$ or $f(z) = e^{-1/z^2}$ at $z = 0$.

**Exercise 2.3** : Write a Newton method to solve $f(z) = 0$ where you compute the gradient of $f$ by ForwardDiff. Test your implementtion with $f(z) = z^2 - 2$. (You can reveal a possible solution by pressing on the eye symbol on the left side of the next hidden box.)

"""

# ╔═╡ 2a54d947-04e7-4b77-8c82-d2ebddadff4a
begin
function newton(f::Function; init = 1.0, maxits = 100, tol = 1e-12)
	zn::Float64 = init
	fzn::Float64 = 0
	dfzn::Float64 = 0
	it::Int = 0
	while (true)
		it += 1
		fzn = f(zn)
		if abs(fzn) < tol
			return zn, fzn, it, true
		elseif it >= maxits
			return zn, fzn, it, false
		else
			dfzn = ForwardDiff.derivative(f,zn)
			if abs(dfzn) < tol
				# Halt due to zero derivative
				return zn, fzn, it, false
			end
			zn = zn - fzn/dfzn
		end
	end
end
res = newton(f; init = z)
if res[4] == true
	Text("After $(res[3]) iterations arrived at...\n\t  z = $(res[1])\n\tf(z)= $(res[2])")
else
	Text("Stopped after $(res[3]) iterations with...\n\t  z = $(res[1])\n\tf(z)= $(res[2])")
end
end

# ╔═╡ 1c7324e7-73d6-41f3-be47-fd794756b842
md"""

### Example 2 : (Partial) Derivatives of vector-valued functions

Let's have a look at some two-dimensionally parametrized vector-field $F : \mathbb{R}^2 \rightarrow \mathbb{R}^3$

"""

# ╔═╡ b86165a5-894a-4f51-ad45-2155ac6bf041
function F(a::Vector{<:Real})
	return [cos(a[1]), sin(a[2]), a[1]^2+a[2]]
end

# ╔═╡ 525771d7-0e1c-4841-adcc-7faf01d72172
md"""

A partial derivative of F with respect to the parameter $y$ could be obtained by

"""

# ╔═╡ 5bc6a8bb-ab37-4bec-82fc-5546cdd2325d
 DF(a) = ForwardDiff.jacobian(F,a)

# ╔═╡ b3f45169-7dc0-4d6f-8d81-22fbe9a91e7e
md"""

Let's evaluate F and DF at some points.

"""

# ╔═╡ be36c18c-9e06-41a6-b76d-22fb2ee359f9
a = [0.0, 0.0] # change the parameters if you like

# ╔═╡ 62a2e422-3ed3-44bf-9d67-11bc899a4b38
Text(" F(a) = $(F(a))\nDF(a) = $(DF(a))")

# ╔═╡ aa74e4e8-8e9f-4c6d-bb2f-4dbf73f88a7e
md"""

Note: $DF(a) \in \mathbb{R}^{3 \times 2}$, the entry
at position $[j,k]$ is the partial derivative of $\partial F_j / \partial a_k$.
Now, let's say we only want to compute the partial derivative of $F_j$ with respect to $a_2$. To achieve that we can define some restricted function by cutting out the part that we want to differentiate:

"""

# ╔═╡ 3fcbd0f1-b767-4876-bb97-9322caf10aae
begin
	j = 3 # you can use any subset of 1:3 here
	F_fixed_a1(a1,j) = a2 -> F([a1,a2])[j]
	dFjda2(j,a) = ForwardDiff.derivative(F_fixed_a1(a[1],j),a[2])
end

# ╔═╡ 4c67d3bf-0d46-4b72-b902-28b31ddcea9a
md""" Let's check if it works when we evaluate dFda2 at the given a (for comparison also the coressponding j-subset of the k-th column of DF(a) is shown)..."""

# ╔═╡ a3ce3eca-150a-402b-ade1-1c391092dbf8
Text("dFjda2(j,a) = $(dFjda2(j,a))\n DF(a)[j,2] = $(DF(a)[j,2])")

# ╔═╡ 73784db5-11d6-4758-946c-b1a065a7d64b
md"""

### Advanced Usage

Whenever a function is differentiated at many points and performance becomes important, it is advisable to use a buffer to store the function value and the desired derivatives. Moreover, if e.g. the hessian is computed it would be nice to get out the lower derivatives as well without issuing another gradient computation. All this can be achieved with using DiffResults. Details can be found in the [Documentation of DiffResults](https://juliadiff.org/DiffResults.jl/stable/)

Additionally, ForwardDiff can be tuned with a Configuration variable to e.g. modify the chunk size which is more relevant for larger input sizes. Details can be found in the [Documentation of ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable/user/advanced/).

Let's try everything with DF(a) above...

"""

# ╔═╡ 2e954113-6e77-4936-83f1-58bb346aeb88
begin
	# construct a DiffResults buffer that stores function values and the jacobian
	# instead of F(a) one can also provide a similar vector (size and type)
	result = DiffResults.JacobianResult(similar(F(a)), a)
	# we also provide a configuration for the derivative
	cfg = ForwardDiff.JacobianConfig(F,a)
	# everything is passed to ForwardDiff.jacobian!
	result = ForwardDiff.jacobian!(result,F,a,cfg)
	# now F(a) and DF(a) can be extracted from result
	Text("F(a) = $(DiffResults.value(result))\nDF(a) = $(DiffResults.gradient(result))")
end

# ╔═╡ 6e721e97-fd41-45d5-bf66-ef6f82caa126
md"""

**Exercise 3** :
Implement an improved Newton method that can be applied to vector-valued functions $G : \mathbb{R}^n \rightarrow \mathbb{R}^n$ and that uses DiffResults.JacobianResult as a buffer. 
"""

# ╔═╡ a28ad4b6-80ec-43a6-8eea-8956d633a5a2
md""" (Reveal the box below to see a possible solution.) """

# ╔═╡ a5b4455b-3d9d-4d98-958f-a3b23e607760
begin
function newton_advanced(F::Function, init; maxits = 100, tol = 1e-12)
	zn::Vector{Float64} = init
	result = DiffResults.JacobianResult(F(zn), zn)
	cfg = ForwardDiff.JacobianConfig(F,zn)
	it::Int = 0
	while (true)
		it += 1
		result = ForwardDiff.jacobian!(result,F,zn,cfg)
		if sqrt(sum(DiffResults.value(result).^2)) < tol
			return zn, DiffResults.value(result), it, true
		elseif it >= maxits
			return zn, DiffResults.value(result), it, false
		elseif any(isnan, zn)
			# Halt due to NaN
			return zn, Diffresults.value(result), it, false
		else
			zn = zn - DiffResults.gradient(result) \ DiffResults.value(result)
		end
	end
end
end

# ╔═╡ 1302357d-2ea7-4efa-a101-952d338135f8
md""" Test your functions with the initial value and function G below """

# ╔═╡ e24bab60-d18b-41f3-99ad-9d7af45fb246
begin
	init = [0.5,0.5,0.5]
	function G(x)
		return x[1] .* [x[1]^2-2, x[2]^2-3, x[3]^2 - 4]
	end

	# call Newton method
	res2 = newton_advanced(G, init)
	if res2[4] == true
		Text("After $(res2[3]) iterations arrived at...\n\t  z = $(res2[1])\n\tf(z)= $(res2[2])")
	else
		Text("Stopped after $(res2[3]) iterations with...\n\t  z = $(res2[1])\n\tf(z)= $(res2[2])")
	end
end

# ╔═╡ 8519854d-7fd9-4b92-b032-4a902a99d12c
md"""



## Reverse differentiation

There is also a [ReverseDiff.jl](https://github.com/JuliaDiff/ReverseDiff.jl) package with a different approach. Here, the function is represented as a directed graph consisting of a series of single operations with known derivatives which then can be multiplied together forward or backward according to the chain rule.

Backward differentiation might be the better alternative for high-dimensional gradients or functions with a number of input parameters that is larger than the output dimension.

### Forward vs. backward pass

To illustrate what is happening in backward differentiation compared to forward differentiation consider the function

$f(x_1,x_2) = x_1x_2 + \sin(x_1)$

and have a look at these two images (taken from [Wikipedia:Automatic_Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)):

"""

# ╔═╡ a089a751-9cc1-4dc0-8725-c84b910ecbcb
begin
	struct Wow
		filename
	end

	function Base.show(io::IO, ::MIME"image/png", w::Wow)
		write(io, read(w.filename))
	end
end

# ╔═╡ 44abd928-22ba-453d-9680-ada271c4078b
Wow("res/forward.png")

# ╔═╡ ff11ab45-8c56-44b4-b2f1-f6d038dc08b6
md"""
In the **forward pass**
- the evaluation of the function is serialised into a directed graph consisting of singular operations (red circles) and their intermediate results $w_k$ with known simple derivatives
- using seeds $\dot w_j = 1$ and going through the graph from bottom to top partial derivatives  $\dot w_k = \partial w_k / \partial {x_j}$
- equivalent to evaluating the chain rule from inside to outside
- that is similar to using Dual numbers !
"""

# ╔═╡ 9bffc36b-9547-4ab3-92c3-405e500addd5
Wow("res/backward.png")

# ╔═╡ dc09ce8c-b27f-4e93-b741-aa1d461e3efa
md"""
In the **backward pass**
- the same graph is traversed backward starting with a seed $\bar f$ for the change of the output
- now in red: $\bar w_k$ denotes the adjoint $\bar w_k = \partial f / \partial w_k$
- equivalent to traversing the chain rule from outside to inside
- hence in the backward pass one seed in $f$ gives the full gradient (instead of two seeds in the forward mode)
- backward pass is better if number of input arguments is larger than number of output arguments, but also needs more memory to store instructions for the backward pass ('tape')
"""

# ╔═╡ 4f4f8076-cc54-40ff-8dee-2c84d46e6e8b
md"""
### Example with ReverseDiff.jl

Finally, let's have at least one small example how to use ReverseDiff.jl for some scalar-valued function $h : \mathbb{R}^{n \times n} \times \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$ that takes two matrices as input.

"""

# ╔═╡ a99b4512-b820-434e-88e5-d58a04689b0c
begin
	n = 3 # set the dimension of A and B
	h(A,B) = sum(A .* B + A' * B + A * B') # modify it if you like
end

# ╔═╡ 466a4c45-5889-4c70-854d-84bbeb013894
md""" Next, we pre-compile the tape for the gradient evaluation """

# ╔═╡ 46070ab8-dd40-4cb2-91a2-28f24813d125
begin
	# record/compile tape for A, B being each nxn matrices
	const h_tape = ReverseDiff.GradientTape(h, (rand(n,n), rand(n,n)))
	const compiled_h_tape = ReverseDiff.compile(h_tape)
end

# ╔═╡ 56b58ecf-87ce-4ed1-a38a-b817f2593ead
md""" Here you can define two matrices to use for the gradient evaluation: """

# ╔═╡ 23ddd62b-5d9c-4101-b97c-f2f09d8b9c79
begin
	A = ones(Float64,n,n)
	B = -3*ones(Float64,n,n)
	A,B
end

# ╔═╡ 847291ec-4fb2-43f0-8838-f6f5fe4fd2f6
md""" And this is the evaluation of $Dh(A,B)=(dhdA,dhdB)=$ """

# ╔═╡ c306dd15-e43a-465f-9671-cb6e496d3de8
begin
	# evaluate gradient of h(inputs) into results
	results = (similar(A), similar(B))
	ReverseDiff.gradient!(results, compiled_h_tape, (A,B))
end

# ╔═╡ a998034d-c2c0-419f-898c-f2cf82814683
md"""

## Hackathon project

In the remaining time we want to work on some project that can be found in the file image_opt.jl.

"""

# ╔═╡ Cell order:
# ╟─d4c4e2f4-ba74-400d-aee1-d5ac2c63ee19
# ╟─5cb08a46-a0f0-11eb-3aaa-03f9763dcb75
# ╟─b19235be-0338-4212-9d0a-5e45a1c866ea
# ╟─688958d3-c39d-488c-9950-2c5fc83dee40
# ╟─c9656131-be14-48f3-9386-8320906ae3e5
# ╟─2029b8bd-408c-4e2f-a628-ee2a56eed43a
# ╠═605fa750-99c8-4ea4-928b-2e13110de9a8
# ╟─9153b452-f073-402d-ac68-a257cb8e7740
# ╠═624a73e1-3ee0-4e7e-a7ec-8d9146d2ac29
# ╟─53a4910c-5e18-4d66-bc34-5712bd0934a9
# ╠═d0bdbfaf-9a8a-408c-aace-5ed53052ab4f
# ╟─e1bbb076-1ec4-4005-a69c-667e7fd0a35b
# ╟─271511b3-c5e5-4ca2-ab79-5372b9f9a616
# ╟─2a54d947-04e7-4b77-8c82-d2ebddadff4a
# ╟─1c7324e7-73d6-41f3-be47-fd794756b842
# ╠═b86165a5-894a-4f51-ad45-2155ac6bf041
# ╟─525771d7-0e1c-4841-adcc-7faf01d72172
# ╠═5bc6a8bb-ab37-4bec-82fc-5546cdd2325d
# ╟─b3f45169-7dc0-4d6f-8d81-22fbe9a91e7e
# ╠═be36c18c-9e06-41a6-b76d-22fb2ee359f9
# ╟─62a2e422-3ed3-44bf-9d67-11bc899a4b38
# ╟─aa74e4e8-8e9f-4c6d-bb2f-4dbf73f88a7e
# ╠═3fcbd0f1-b767-4876-bb97-9322caf10aae
# ╟─4c67d3bf-0d46-4b72-b902-28b31ddcea9a
# ╟─a3ce3eca-150a-402b-ade1-1c391092dbf8
# ╟─73784db5-11d6-4758-946c-b1a065a7d64b
# ╠═2e954113-6e77-4936-83f1-58bb346aeb88
# ╟─6e721e97-fd41-45d5-bf66-ef6f82caa126
# ╟─a28ad4b6-80ec-43a6-8eea-8956d633a5a2
# ╟─a5b4455b-3d9d-4d98-958f-a3b23e607760
# ╟─1302357d-2ea7-4efa-a101-952d338135f8
# ╟─e24bab60-d18b-41f3-99ad-9d7af45fb246
# ╟─8519854d-7fd9-4b92-b032-4a902a99d12c
# ╟─a089a751-9cc1-4dc0-8725-c84b910ecbcb
# ╟─44abd928-22ba-453d-9680-ada271c4078b
# ╟─ff11ab45-8c56-44b4-b2f1-f6d038dc08b6
# ╟─9bffc36b-9547-4ab3-92c3-405e500addd5
# ╟─dc09ce8c-b27f-4e93-b741-aa1d461e3efa
# ╟─4f4f8076-cc54-40ff-8dee-2c84d46e6e8b
# ╠═a99b4512-b820-434e-88e5-d58a04689b0c
# ╟─466a4c45-5889-4c70-854d-84bbeb013894
# ╠═46070ab8-dd40-4cb2-91a2-28f24813d125
# ╟─56b58ecf-87ce-4ed1-a38a-b817f2593ead
# ╠═23ddd62b-5d9c-4101-b97c-f2f09d8b9c79
# ╟─847291ec-4fb2-43f0-8838-f6f5fe4fd2f6
# ╠═c306dd15-e43a-465f-9671-cb6e496d3de8
# ╟─a998034d-c2c0-419f-898c-f2cf82814683
