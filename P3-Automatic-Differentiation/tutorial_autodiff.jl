### A Pluto.jl notebook ###
# v0.14.2

using Markdown
using InteractiveUtils

# ╔═╡ 5cb08a46-a0f0-11eb-3aaa-03f9763dcb75
begin
	using ForwardDiff
	using DiffResults
	using PlutoUI
	PlutoUI.TableOfContents()
end

# ╔═╡ d4c4e2f4-ba74-400d-aee1-d5ac2c63ee19
md"""

# Automatic Differentiation with Julia

## Introduction

This tutorial gives some first insight into the automatic differentiation features (mainly ForwardDiff.jl) of Julia. To run it properly you need to have installed the following packages:

- [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl)
- [DiffResults](https://github.com/JuliaDiff/DiffResults.jl)

On the right-hand side you should see a table of contents.

"""

# ╔═╡ b19235be-0338-4212-9d0a-5e45a1c866ea
md"""

## Dual Numbers

Forward differentiation is based on dual numbers. Dual numbers look like complex numbers as they are written as

$ z = a + b \epsilon \quad \text{where} \quad \epsilon^2 = 0 $

and have a 'real part' $a$ and a 'dual part' $b$ being the coefficient for the 'dual unit' $\epsilon$.

You can add and multiply them commutatively by

$ (a + b \epsilon) + (x + y \epsilon) = (a + x) + \epsilon (b + y) $

$ (a + b \epsilon) \cdot (x + y \epsilon) = (ax + 0) + \epsilon (ay + bx) $

"""

# ╔═╡ 688958d3-c39d-488c-9950-2c5fc83dee40
md"""

### Taylor series

Consider a (smooth) function $f : \mathbb{R} \rightarrow \mathbb{R}$ and its Taylor series expansion at some value $a$:

$ f(a+h) = f(a) + \sum_{n=1}^\infty \frac{f^{(n)}(a)}{n!} h^n $

Now, for $h = \epsilon$, due to $\epsilon^2 = 0$ one immediately obtains

$ f(a+h) = f(a) + f^{(1)}(a) \epsilon $

In other words: Evaluating the function in the dual number $(a + 1\epsilon)$ gives the dual number $(f(a) + f^{(1)}(a) \epsilon)$ as a result.

**Key point**: If the function $f$ can be also evaluated in dual numbers, one can obtain its derivatives 'automatically' on basis of the dual number operations above!
Due to Julia's on demand compilation and type dispatching, there will be two compiled versions of f(x<:Real), one for e.g. x::Float64 and one for the corressponding dual numbers (which is still a subtype of Real).

Details on the actual implementation of the dual numbers can be found in the [Documentation of ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable/dev/how_it_works/)

**Exercise**: Compute the derivative of $f(x) = (x-1)(x-2)$ at $x = 3$ by hand using dual numbers, i.e. compute $f(3 + \epsilon)$.

"""

# ╔═╡ c9656131-be14-48f3-9386-8320906ae3e5
begin
	Text("Reveal this cell completely to see the solution to the exercise above.")
	#
	# f(3+ϵ) = (2+ϵ)(1+ϵ) = 2 + 3ϵ + ϵ^2 = 3 + 3ϵ
	#
	# Hence, f(3) = 2 and df(3) = 3
	#
	# For arbitrary x one obtains
	# f(x+ϵ) = (x+ϵ-1)(x+ϵ-2) = x^2-3x+2 + ϵ(2x-3)
	#
	# Hence, df(x) = 2x-3
	#
end

# ╔═╡ 2029b8bd-408c-4e2f-a628-ee2a56eed43a
md"""

### Example 1 : 1D differentiation
Let's define some scalar-valued function $f : \mathbb{R} \rightarrow \mathbb{R}$ to test this:

"""

# ╔═╡ 605fa750-99c8-4ea4-928b-2e13110de9a8
function f(z::Real)
	return cos(z)*sin(z) + z^2 - 1 # change the function here if you like
end

# ╔═╡ 9153b452-f073-402d-ac68-a257cb8e7740
md"""

The first and second derivatives of our function f can be generated by the following lines:

"""

# ╔═╡ 624a73e1-3ee0-4e7e-a7ec-8d9146d2ac29
begin
	df(z) = ForwardDiff.derivative(f,z)
	d2f(z) = ForwardDiff.derivative(df,z)
end

# ╔═╡ 53a4910c-5e18-4d66-bc34-5712bd0934a9
md"""

Let's evaluate f and its derivatives at some point z.

"""

# ╔═╡ d0bdbfaf-9a8a-408c-aace-5ed53052ab4f
z = pi # change the evaluation point here if you like

# ╔═╡ e1bbb076-1ec4-4005-a69c-667e7fd0a35b
Text("  f(z) = $(f(z))\n df(z) = $(df(z))\nd2f(z) = $(d2f(z))")

# ╔═╡ 271511b3-c5e5-4ca2-ab79-5372b9f9a616
md"""

### Exercise Block 1

**Exercise 1.1** : Compute the derivative of the function $f(z) = (z+1)\exp(-z^2)$ at z = 1. (You just have to manipulate the function f and z above.) Also check what you have computed by hand in the dual number exercise above.

**Exercise 1.2** : Try discontinuous functions to see what happens, e.g. $f(z) = \lvert z \rvert$ around $z = 0$ or $f(z) = e^{-1/z^2}$ at $z = 0$.

**Exercise 1.3** : Write a Newton method to solve $f(z) = 0$ where you compute the gradient of $f$ by ForwardDiff. Test your implementtion with $f(z) = z^2 - 2$. (You can reveal the Solution by pressing on the eye symbol on the left side of the next hidden box.)

"""

# ╔═╡ 2a54d947-04e7-4b77-8c82-d2ebddadff4a
begin
function newton(f::Function; init = 1.0, maxits = 100, tol = 1e-12)
	zn::Float64 = init
	fzn::Float64 = 0
	dfzn::Float64 = 0
	it::Int = 0
	while (true)
		it += 1
		fzn = f(zn)
		if abs(fzn) < tol
			return zn, fzn, it, true
		elseif it >= maxits
			return zn, fzn, it, false
		else
			dfzn = ForwardDiff.derivative(f,zn)
			if abs(dfzn) < tol
				# Halt due to zero derivative
				return zn, fzn, it, false
			end
			zn = zn - fzn/dfzn
		end
	end
end
res = newton(f; init = z)
if res[4] == true
	Text("After $(res[3]) iterations arrived at...\n\t  z = $(res[1])\n\tf(z)= $(res[2])")
else
	Text("Stopped after $(res[3]) iterations with...\n\t  z = $(res[1])\n\tf(z)= $(res[2])")
end
end

# ╔═╡ 1c7324e7-73d6-41f3-be47-fd794756b842
md"""

### Example 2 : (Partial) Derivatives of vector-valued functions

Let's have a look at some two-dimensionally parametrized vector-field $F : \mathbb{R}^2 \rightarrow \mathbb{R}^3$

"""

# ╔═╡ b86165a5-894a-4f51-ad45-2155ac6bf041
function F(a::Vector{<:Real})
	return [cos(a[1]), sin(a[2]), a[1]^2+a[2]]
end

# ╔═╡ 525771d7-0e1c-4841-adcc-7faf01d72172
md"""

A partial derivative of F with respect to the parameter $y$ could be obtained by

"""

# ╔═╡ 5bc6a8bb-ab37-4bec-82fc-5546cdd2325d
 DF(a) = ForwardDiff.jacobian(F,a)

# ╔═╡ b3f45169-7dc0-4d6f-8d81-22fbe9a91e7e
md"""

Let's evaluate F and DF at some points.

"""

# ╔═╡ be36c18c-9e06-41a6-b76d-22fb2ee359f9
a = [0.0, 0.0] # change the parameters if you like

# ╔═╡ 62a2e422-3ed3-44bf-9d67-11bc899a4b38
Text(" F(a) = $(F(a))\nDF(a) = $(DF(a))")

# ╔═╡ aa74e4e8-8e9f-4c6d-bb2f-4dbf73f88a7e
md"""

Note: $DF(a) \in \mathbb{R}^{3 \times 2}$, the entry
at position $[j,k]$ is the partial derivative of $\partial F_j / \partial a_k$.
Now, let's say we only want to compute the partial derivative of $F_j$ with respect to $a_2$. To achieve that we can define some restricted function by cutting out the part that we want to differentiate:

"""

# ╔═╡ 3fcbd0f1-b767-4876-bb97-9322caf10aae
begin
	j = 3 # you can use any subset of 1:3 here
	F_fixed_a1(a1,j) = a2 -> F([a1,a2])[j]
	dFjda2(j,a) = ForwardDiff.derivative(F_fixed_a1(a[1],j),a[2])
end

# ╔═╡ 4c67d3bf-0d46-4b72-b902-28b31ddcea9a
md""" Let's check if it works when we evaluate dFda2 at the given a (for comparison also the coressponding j-subset of the k-th column of DF(a) is shown)..."""

# ╔═╡ a3ce3eca-150a-402b-ade1-1c391092dbf8
Text("dFjda2(j,a) = $(dFjda2(j,a))\n DF(a)[j,2] = $(DF(a)[j,2])")

# ╔═╡ 73784db5-11d6-4758-946c-b1a065a7d64b
md"""

## Advanced Usage

Whenever a function is differentiated at many points and performance becomes important, it is advisable to use a buffer to store the function value and the desired derivatives. Moreover, if e.g. the hessian is computed it would be nice to get out the lower derivatives as well without issuing another gradient computation. All this can be achieved with using DiffResults. Details can be found in the [Documentation of DiffResults](https://juliadiff.org/DiffResults.jl/stable/)

Additionally, ForwardDiff can be tuned with a Configuration variable to e.g. modify the chunk size which is more relevant for larger input sizes. Details can be found in the [Documentation of ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable/user/advanced/).

Let's try everything with DF(a) above...

"""

# ╔═╡ 2e954113-6e77-4936-83f1-58bb346aeb88
begin
	# construct a DiffResults buffer that stores function values and the jacobian
	# instead of F(a) one can also provide a similar vector (size and type)
	result = DiffResults.JacobianResult(similar(F(a)), a)
	# we also provide a configuration for the derivative
	cfg = ForwardDiff.JacobianConfig(F,a)
	# everything is passed to ForwardDiff.jacobian!
	result = ForwardDiff.jacobian!(result,F,a,cfg)
	# now F(a) and DF(a) can be extracted from result
	Text("F(a) = $(DiffResults.value(result))\nDF(a) = $(DiffResults.gradient(result))")
end

# ╔═╡ 6e721e97-fd41-45d5-bf66-ef6f82caa126
md"""

### Exercise Block 2

**Exercise 2.1** : Rewrite your Newton method so that it handles vector-valued functions $G : \mathbb{R}^n \rightarrow \mathbb{R}^n$ and use DiffResults.JacobianResult as a buffer. (Reveal the box below to see a possible solution.)

"""

# ╔═╡ 8ba620fc-b84c-4f62-a2c4-deb7c49b6e67
begin
	init = [0.5,0.5,0.5]
	function G(x)
		return x[1] .* [x[1]^2-2,x[2]^2-3,x[3]^2 - 4]
	end
	Text("Enter initial guess and function G here:")
end

# ╔═╡ a5b4455b-3d9d-4d98-958f-a3b23e607760
begin
function newton_advanced(F::Function, init; maxits = 100, tol = 1e-12)
	zn::Vector{Float64} = init
	result = DiffResults.JacobianResult(F(zn), zn)
	cfg = ForwardDiff.JacobianConfig(F,zn)
	it::Int = 0
	while (true)
		it += 1
		result = ForwardDiff.jacobian!(result,F,zn,cfg)
		if sqrt(sum(DiffResults.value(result).^2)) < tol
			return zn, DiffResults.value(result), it, true
		elseif it >= maxits
			return zn, DiffResults.value(result), it, false
		elseif any(isnan, zn)
			# Halt due to NaN
			return zn, Diffresults.value(result), it, false
		else
			zn = zn - DiffResults.gradient(result) \ DiffResults.value(result)
		end
	end
end
res2 = newton_advanced(G, init)
if res2[4] == true
	Text("After $(res2[3]) iterations arrived at...\n\t  z = $(res2[1])\n\tf(z)= $(res2[2])")
else
	Text("Stopped after $(res2[3]) iterations with...\n\t  z = $(res2[1])\n\tf(z)= $(res2[2])")
end
end

# ╔═╡ 8519854d-7fd9-4b92-b032-4a902a99d12c
md"""

## Final Remarks


### Limitations of ForwardDiff.jl

There are some more or less obvious limitations to ForwardDiff:

- you can only use it only generic Julia functions
- the function must be generic enough to accept <:Real arguments
- ... ???


### ReverseDiff.jl, Zygote.jl ...

There is also a [ReverseDiff.jl](https://github.com/JuliaDiff/ReverseDiff.jl) package with a completely different approach. In the reverse mode the function is first split into a 'forward pass' which is a series of single operations with known derivatives which are then multiplied together according to the chain rule (called 'reverse pass').

It might be the better alternative for high-dimensional gradients or functions with a number of input parameters that is larger than the output dimension. Another reverse mode automatic differentiation package is [Zygote.jl](https://github.com/FluxML/Zygote.jl) that is used in the FluxML community.

"""

# ╔═╡ Cell order:
# ╟─d4c4e2f4-ba74-400d-aee1-d5ac2c63ee19
# ╟─5cb08a46-a0f0-11eb-3aaa-03f9763dcb75
# ╟─b19235be-0338-4212-9d0a-5e45a1c866ea
# ╟─688958d3-c39d-488c-9950-2c5fc83dee40
# ╟─c9656131-be14-48f3-9386-8320906ae3e5
# ╟─2029b8bd-408c-4e2f-a628-ee2a56eed43a
# ╠═605fa750-99c8-4ea4-928b-2e13110de9a8
# ╟─9153b452-f073-402d-ac68-a257cb8e7740
# ╠═624a73e1-3ee0-4e7e-a7ec-8d9146d2ac29
# ╟─53a4910c-5e18-4d66-bc34-5712bd0934a9
# ╠═d0bdbfaf-9a8a-408c-aace-5ed53052ab4f
# ╟─e1bbb076-1ec4-4005-a69c-667e7fd0a35b
# ╟─271511b3-c5e5-4ca2-ab79-5372b9f9a616
# ╟─2a54d947-04e7-4b77-8c82-d2ebddadff4a
# ╟─1c7324e7-73d6-41f3-be47-fd794756b842
# ╠═b86165a5-894a-4f51-ad45-2155ac6bf041
# ╟─525771d7-0e1c-4841-adcc-7faf01d72172
# ╠═5bc6a8bb-ab37-4bec-82fc-5546cdd2325d
# ╟─b3f45169-7dc0-4d6f-8d81-22fbe9a91e7e
# ╠═be36c18c-9e06-41a6-b76d-22fb2ee359f9
# ╟─62a2e422-3ed3-44bf-9d67-11bc899a4b38
# ╟─aa74e4e8-8e9f-4c6d-bb2f-4dbf73f88a7e
# ╠═3fcbd0f1-b767-4876-bb97-9322caf10aae
# ╟─4c67d3bf-0d46-4b72-b902-28b31ddcea9a
# ╟─a3ce3eca-150a-402b-ade1-1c391092dbf8
# ╟─73784db5-11d6-4758-946c-b1a065a7d64b
# ╟─2e954113-6e77-4936-83f1-58bb346aeb88
# ╟─6e721e97-fd41-45d5-bf66-ef6f82caa126
# ╠═8ba620fc-b84c-4f62-a2c4-deb7c49b6e67
# ╟─a5b4455b-3d9d-4d98-958f-a3b23e607760
# ╟─8519854d-7fd9-4b92-b032-4a902a99d12c
